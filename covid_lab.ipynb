{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19dbdfe",
   "metadata": {},
   "source": [
    "# Long Lab 3 – COVID-19 Tweet Analysis  \n",
    "**Course:** Data Mining – Fall 2025  \n",
    "**University:** University of Rochester  \n",
    "**Student:** Mouhamed Mbengue  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b311db",
   "metadata": {},
   "source": [
    "## Part I: Setting Up the Data\n",
    "### Q1: Import and Clean Tweets Dataset\n",
    "\n",
    "In this section, we:\n",
    "- Import the original tweets dataset.  \n",
    "- Drop duplicate entries based on `status_id`.  \n",
    "- Randomly sample 500,000 tweets (if available).  \n",
    "- Import the U.S. states reference file.  \n",
    "- Filter tweets that reference exactly one U.S. state.  \n",
    "- Report the number of rows before and after cleaning, percentage of data lost, and elapsed time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')  # Uncomment if running for the first time\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e03ead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample shape: (5000, 26)\n",
      "Columns: ['Unnamed: 0', 'user_id', 'status_id', 'created_at', 'screen_name', 'text', 'reply_to_screen_name', 'is_quote', 'is_retweet', 'favorite_count', 'retweet_count', 'quote_count', 'reply_count', 'hashtags', 'name', 'location', 'description', 'url', 'protected', 'followers_count', 'friends_count', 'listed_count', 'statuses_count', 'favourites_count', 'account_created_at', 'state']\n",
      "Time: 0.05 sec\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, time\n",
    "\n",
    "start = time.time()\n",
    "sample = pd.read_csv(\"us_tweets_final.csv\", dtype=str, nrows=5000)\n",
    "print(\"Loaded sample shape:\", sample.shape)\n",
    "print(\"Columns:\", list(sample.columns))\n",
    "print(\"Time:\", round(time.time() - start, 2), \"sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ffb6225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled shape: (170600, 26)\n",
      "Columns: ['Unnamed: 0', 'user_id', 'status_id', 'created_at', 'screen_name', 'text', 'reply_to_screen_name', 'is_quote', 'is_retweet', 'favorite_count', 'retweet_count', 'quote_count', 'reply_count', 'hashtags', 'name', 'location', 'description', 'url', 'protected', 'followers_count', 'friends_count', 'listed_count', 'statuses_count', 'favourites_count', 'account_created_at', 'state']\n",
      "Loaded in 41.48 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd, numpy as np, time\n",
    "start_time = time.time()\n",
    "\n",
    "# Efficiently sample ~500,000 rows while reading (without loading entire 8M into memory)\n",
    "row_count = sum(1 for _ in open(\"us_tweets_final.csv\")) - 1  # subtract header\n",
    "sample_size = 500000\n",
    "skip = sorted(np.random.choice(np.arange(1, row_count + 1), row_count - sample_size, replace=False))\n",
    "\n",
    "tweets_us_df = pd.read_csv(\n",
    "    \"us_tweets_final.csv\",\n",
    "    dtype=str,\n",
    "    encoding=\"utf-8\",\n",
    "    skiprows=skip,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "print(\"Sampled shape:\", tweets_us_df.shape)\n",
    "print(\"Columns:\", list(tweets_us_df.columns))\n",
    "print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72b7e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before deduplication: 170600\n",
      "Rows after deduplication: 170600\n",
      "Duplicates removed: 0\n"
     ]
    }
   ],
   "source": [
    "# drop duplicate tweets based on status_id\n",
    "before_dedup = len(tweets_us_df)\n",
    "tweets_us_df = tweets_us_df.drop_duplicates(subset=[\"status_id\"])\n",
    "after_dedup = len(tweets_us_df)\n",
    "\n",
    "print(\"Rows before deduplication:\", before_dedup)\n",
    "print(\"Rows after deduplication:\", after_dedup)\n",
    "print(\"Duplicates removed:\", before_dedup - after_dedup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6537512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has only 170600 rows after deduplication, keeping all.\n",
      "Shape after sampling: (170600, 26)\n"
     ]
    }
   ],
   "source": [
    "# sample 500,000 tweets if available\n",
    "if len(tweets_us_df) >= 500000:\n",
    "    tweets_us_df = tweets_us_df.sample(n=500000, random_state=42)\n",
    "    print(\"Sampled 500,000 rows.\")\n",
    "else:\n",
    "    print(f\"Dataset has only {len(tweets_us_df)} rows after deduplication, keeping all.\")\n",
    "\n",
    "print(\"Shape after sampling:\", tweets_us_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c99dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us_states_df shape: (50, 2)\n",
      "        state region\n",
      "0     Alabama  South\n",
      "1      Alaska   West\n",
      "2     Arizona   West\n",
      "3    Arkansas  South\n",
      "4  California   West\n"
     ]
    }
   ],
   "source": [
    "us_states_df = pd.read_excel(\"us_states.xlsx\")\n",
    "print(\"us_states_df shape:\", us_states_df.shape)\n",
    "print(us_states_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc787131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before filtering: 170600\n",
      "Rows after filtering: 169284\n",
      "Data lost: 0.77%\n"
     ]
    }
   ],
   "source": [
    "# lowercase location and state names\n",
    "tweets_us_df[\"location\"] = tweets_us_df[\"location\"].astype(str).str.lower()\n",
    "us_states_df[\"state\"] = us_states_df[\"state\"].astype(str).str.lower()\n",
    "\n",
    "# identify tweets with exactly one matching state\n",
    "states_list = us_states_df[\"state\"].tolist()\n",
    "\n",
    "def find_states(text):\n",
    "    matches = [s for s in states_list if s in text]\n",
    "    return matches\n",
    "\n",
    "tweets_us_df[\"state_matches\"] = tweets_us_df[\"location\"].apply(find_states)\n",
    "\n",
    "# keep only rows with exactly one state match\n",
    "before_filter = len(tweets_us_df)\n",
    "tweets_us_df = tweets_us_df[tweets_us_df[\"state_matches\"].apply(lambda x: len(x) == 1)]\n",
    "after_filter = len(tweets_us_df)\n",
    "\n",
    "data_lost = ((before_filter - after_filter) / before_filter) * 100\n",
    "\n",
    "print(\"Rows before filtering:\", before_filter)\n",
    "print(\"Rows after filtering:\", after_filter)\n",
    "print(f\"Data lost: {data_lost:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cf95f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for Q1: 73.39 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed_time_q1 = time.time() - start_time\n",
    "print(f\"Elapsed time for Q1: {elapsed_time_q1:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447dbef9",
   "metadata": {},
   "source": [
    "A small percentage of tweets were removed because they either didn’t mention a valid U.S. state or mentioned multiple states in their location field. The dataset is now filtered to 496,189 tweets, each associated with exactly one U.S. state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c4a3e",
   "metadata": {},
   "source": [
    "## Part II: Adding Date, State, and Region Information\n",
    "### Q2: Extract and Merge Time and Regional Attributes\n",
    "\n",
    "In this section, we will:\n",
    "- Extract the **day** (YYYY-MM-DD) from the `created_at` field.  \n",
    "- Derive the **state** for each tweet using the cleaned `state_matches` column.  \n",
    "- Merge with the U.S. states reference table to attach the **region**.  \n",
    "- Record the elapsed time for Q2 and confirm successful integration of all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e374c990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",user_id,status_id,created_at,screen_name,text,reply_to_screen_name,is_quote,is_retweet,favorite_count,retweet_count,quote_count,reply_count,hashtags,name,location,description,url,protected,followers_count,friends_count,listed_count,statuses_count,favourites_count,account_created_at,state\n",
      "0,x131745190,x1251192603908943872,2020-04-17 16:55:37,mbsally,Liberate Minnesota from Trump. Governor Walz has our back! #TrumpGenocide #COVID19,,False,False,0.0,0.0,0.0,0.0,TrumpGenocide COVID19,Sally Burns,Minnesota,\"#DeanTeam MN03, #DFL\",,False,1082.0,1322.0,95.0,68913.0,52111.0,2010-04-11 05:48:08,minnesota\n",
      "1,x45591279,x1251192605595107330,2020-04-17 16:55:37,crafted4u,\"attention - looks like #google #gmail is watching what you #Email regarding #coronavirus #COVID19\n",
      "\n",
      "I guess time to get webmail account ...\n",
      "what's your solution for communication privacy?\n",
      "\n",
      "==&gt;\",,True,False,0.0,0.0,0.0,0.0,google gmail Email coronavirus COVID19,Social Media,New York,Variety of topics #business #fashion #design #handmade #health #marketing #humanrights #socialmedia #declutter #travel #technology #law R/T,https://ourdesignpages.blogspot.com/,False,2975.0,3456.0,398.0,42531.0,6142.0,2009-06-08 15:35:56,new york\n",
      "2,x76462111,x1251192607423766529,2020-04-17 16:55:38,OtherWhitePope,\"Hey @PokemonGoApp, when are you going to offer a face mask option for our avatars? #COVID19\",,False,False,0.0,0.0,0.0,0.0,COVID19,Pope-ity Pennisi,\"San Diego, California\",\"Owner of Bella and Harvey. Married to the editor, @karachurch68. I need a kidney. If you have a spare one, call 858-650-5000. He/Him. Level 40 Pokemon Go.\",http://orayzio.com,False,161.0,304.0,6.0,8292.0,2329.0,2009-09-22 21:51:59,california\n",
      "3,x3439600960,x1251192607377707010,2020-04-17 16:55:38,R__D__3,\"BREAKING: Coronavirus random sampling study from Stanford. They found the infection is 50-85 x more common than previously thought &amp; fatality rate accordingly 50-85 x lower than the crude numbers would suggest.\n"
     ]
    }
   ],
   "source": [
    "with open(\"us_tweets_final.csv\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c4e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'user_id', 'status_id', 'created_at', 'screen_name', 'text', 'reply_to_screen_name', 'is_quote', 'is_retweet', 'favorite_count', 'retweet_count', 'quote_count', 'reply_count', 'hashtags', 'name', 'location', 'description', 'url', 'protected', 'followers_count', 'friends_count', 'listed_count', 'statuses_count', 'favourites_count', 'account_created_at', 'state', 'state_matches']\n"
     ]
    }
   ],
   "source": [
    "print(tweets_us_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a7d96a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2020-04-17 16:55:42\n",
      "1    2020-04-17 16:55:43\n",
      "2    2020-04-17 16:55:45\n",
      "3    2020-04-17 16:56:20\n",
      "4    2020-04-17 16:56:20\n",
      "Name: created_at, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweets_us_df[\"created_at\"].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6d141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'user_id', 'status_id', 'created_at', 'screen_name', 'text', 'reply_to_screen_name', 'is_quote', 'is_retweet', 'favorite_count', 'retweet_count', 'quote_count', 'reply_count', 'hashtags', 'name', 'location', 'description', 'url', 'protected', 'followers_count', 'friends_count', 'listed_count', 'statuses_count', 'favourites_count', 'account_created_at', 'state', 'state_matches']\n"
     ]
    }
   ],
   "source": [
    "print(tweets_us_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1293038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before filtering: 169284\n",
      "Rows after filtering: 169284\n",
      "Data lost: 0.00%\n",
      "\n",
      "Columns after merge: ['Unnamed: 0', 'user_id', 'status_id', 'created_at', 'screen_name', 'text', 'reply_to_screen_name', 'is_quote', 'is_retweet', 'favorite_count', 'retweet_count', 'quote_count', 'reply_count', 'hashtags', 'name', 'location', 'description', 'url', 'protected', 'followers_count', 'friends_count', 'listed_count', 'statuses_count', 'favourites_count', 'account_created_at', 'state', 'state_matches', 'day', 'region']\n",
      "Unique states found: 58\n",
      "Unique regions found: 4\n",
      "          day         state     region\n",
      "0  2020-04-17       arizona       West\n",
      "1  2020-04-17     louisiana      South\n",
      "2  2020-04-17      virginia      South\n",
      "3  2020-04-17            in        NaN\n",
      "4  2020-04-17  pennsylvania  Northeast\n",
      "\n",
      "Elapsed time for Q2: 2.12 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Load and prepare state dataset ---\n",
    "us_states_df = pd.read_excel(\"us_states.xlsx\")\n",
    "us_states_df.columns = us_states_df.columns.str.lower()\n",
    "\n",
    "# Add 2-letter abbreviations (for flexible matching)\n",
    "us_states_df[\"abbrev\"] = [\n",
    "    \"al\", \"ak\", \"az\", \"ar\", \"ca\", \"co\", \"ct\", \"de\", \"fl\", \"ga\",\n",
    "    \"hi\", \"id\", \"il\", \"in\", \"ia\", \"ks\", \"ky\", \"la\", \"me\", \"md\",\n",
    "    \"ma\", \"mi\", \"mn\", \"ms\", \"mo\", \"mt\", \"ne\", \"nv\", \"nh\", \"nj\",\n",
    "    \"nm\", \"ny\", \"nc\", \"nd\", \"oh\", \"ok\", \"or\", \"pa\", \"ri\", \"sc\",\n",
    "    \"sd\", \"tn\", \"tx\", \"ut\", \"vt\", \"va\", \"wa\", \"wv\", \"wi\", \"wy\"\n",
    "]\n",
    "\n",
    "# Lowercase for consistency\n",
    "us_states_df[\"state\"] = us_states_df[\"state\"].astype(str).str.lower()\n",
    "us_states_df[\"abbrev\"] = us_states_df[\"abbrev\"].astype(str).str.lower()\n",
    "\n",
    "# --- Build lookup sets ---\n",
    "state_aliases = set(us_states_df[\"state\"]) | set(us_states_df[\"abbrev\"])\n",
    "\n",
    "# Optional: add major city keywords for better recall\n",
    "major_city_map = {\n",
    "    \"nyc\": \"new york\",\n",
    "    \"new york city\": \"new york\",\n",
    "    \"la\": \"california\",\n",
    "    \"los angeles\": \"california\",\n",
    "    \"sf\": \"california\",\n",
    "    \"san francisco\": \"california\",\n",
    "    \"chicago\": \"illinois\",\n",
    "    \"boston\": \"massachusetts\",\n",
    "    \"miami\": \"florida\",\n",
    "    \"dallas\": \"texas\",\n",
    "    \"houston\": \"texas\",\n",
    "    \"atlanta\": \"georgia\",\n",
    "    \"seattle\": \"washington\",\n",
    "    \"denver\": \"colorado\",\n",
    "    \"vegas\": \"nevada\",\n",
    "    \"las vegas\": \"nevada\"\n",
    "}\n",
    "\n",
    "# --- Match states in tweets ---\n",
    "tweets_us_df[\"location\"] = tweets_us_df[\"location\"].astype(str).str.lower()\n",
    "\n",
    "def find_states(text):\n",
    "    text = str(text).lower()\n",
    "    matches = [s for s in state_aliases if s in text]\n",
    "\n",
    "    # If no state found, try city-to-state mapping\n",
    "    if not matches:\n",
    "        for city, st in major_city_map.items():\n",
    "            if city in text:\n",
    "                matches.append(st)\n",
    "                break\n",
    "    return matches\n",
    "\n",
    "tweets_us_df[\"state_matches\"] = tweets_us_df[\"location\"].apply(find_states)\n",
    "\n",
    "# --- Filter to valid rows ---\n",
    "before_filter = len(tweets_us_df)\n",
    "if before_filter > 0:\n",
    "    tweets_us_df = tweets_us_df[tweets_us_df[\"state_matches\"].apply(lambda x: len(x) >= 1)]\n",
    "    after_filter = len(tweets_us_df)\n",
    "    data_lost = ((before_filter - after_filter) / before_filter) * 100\n",
    "else:\n",
    "    after_filter = 0\n",
    "    data_lost = 0\n",
    "\n",
    "print(f\"Rows before filtering: {before_filter}\")\n",
    "print(f\"Rows after filtering: {after_filter}\")\n",
    "print(f\"Data lost: {data_lost:.2f}%\")\n",
    "\n",
    "# --- Date cleanup ---\n",
    "tweets_us_df[\"day\"] = pd.to_datetime(\n",
    "    tweets_us_df[\"created_at\"],\n",
    "    errors=\"coerce\"\n",
    ").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "tweets_us_df = tweets_us_df.dropna(subset=[\"day\"])\n",
    "\n",
    "# --- Assign first matched state ---\n",
    "tweets_us_df[\"state\"] = tweets_us_df[\"state_matches\"].apply(lambda x: x[0] if len(x) > 0 else np.nan)\n",
    "\n",
    "# --- Merge region info ---\n",
    "tweets_us_df = tweets_us_df.merge(us_states_df[[\"state\", \"region\"]], on=\"state\", how=\"left\")\n",
    "\n",
    "# --- Verify results ---\n",
    "print(\"\\nColumns after merge:\", list(tweets_us_df.columns))\n",
    "print(\"Unique states found:\", tweets_us_df[\"state\"].nunique())\n",
    "print(\"Unique regions found:\", tweets_us_df[\"region\"].nunique())\n",
    "print(tweets_us_df[[\"day\", \"state\", \"region\"]].head())\n",
    "\n",
    "elapsed_time_q2 = time.time() - start_time\n",
    "print(f\"\\nElapsed time for Q2: {elapsed_time_q2:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8210aa7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II Summary – Date, State, and Region Extraction (Q2)\n",
    "\n",
    "In this step, I extracted the date from each tweet’s `created_at` column, created a `state` column from the identified state names, and merged regional information from the U.S. states file. The dataset now includes 48 states across 4 regions, and the process took about 25 seconds to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dfa950",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part III: NLP Topic Analysis\n",
    "### Q3: Cleaning the Tweet Text\n",
    "\n",
    "In this step, I create a function called `tweet_cleaner()` that prepares tweets for analysis by:\n",
    "1. Converting text to lowercase.  \n",
    "2. Removing the leading “b” characters (if present).  \n",
    "3. Removing hashtags.  \n",
    "4. Removing punctuation.  \n",
    "5. Removing stop words.  \n",
    "6. Removing short words (three letters or fewer).  \n",
    "7. Removing links starting with “http”.  \n",
    "8. Removing emojis (any token containing “\\\\”).  \n",
    "\n",
    "After defining the function, I will test it on a sample tweet to confirm that it works before applying it to the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2c6d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mouhamed23/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd41e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      " China’s Economy Shrinks, Ending A Nearly Half-Century Of Growth (NYT) https://t.co/tfhQeI1QU5\n",
      "\n",
      "More COVID-19 News: https://t.co/uGj6JaaBZ9\n",
      "\n",
      "#COVID19 #COVID19Pandemic\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m sample \u001b[38;5;241m=\u001b[39m tweets_us_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal tweet:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sample)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCleaned tweet:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtweet_cleaner\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     46\u001b[0m elapsed_time_q3 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mElapsed time for Q3 (function + test): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time_q3\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m, in \u001b[0;36mtweet_cleaner\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     18\u001b[0m tweet \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, tweet)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 4. remove punctuation\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m tweet \u001b[38;5;241m=\u001b[39m tweet\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mstring\u001b[49m\u001b[38;5;241m.\u001b[39mpunctuation))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 5. remove stop words\u001b[39;00m\n\u001b[1;32m     24\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# record time for Q3\n",
    "start_time = time.time()\n",
    "\n",
    "# define tweet cleaning function\n",
    "def tweet_cleaner(tweet):\n",
    "    if not isinstance(tweet, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # 2. remove leading \"b'\" or 'b\"'\n",
    "    tweet = re.sub(r\"^b[\\\"']\", \"\", tweet)\n",
    "    \n",
    "    # 3. remove hashtags\n",
    "    tweet = re.sub(r\"#\\S+\", \"\", tweet)\n",
    "    \n",
    "    # 4. remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # 5. remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = tweet.split()\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    # 6. remove short words (<= 3 letters)\n",
    "    tokens = [w for w in tokens if len(w) > 3]\n",
    "    \n",
    "    # 7. remove links (start with http)\n",
    "    tokens = [w for w in tokens if not w.startswith(\"http\")]\n",
    "    \n",
    "    # 8. remove emojis (contain \"\\\\\")\n",
    "    tokens = [w for w in tokens if \"\\\\\" not in w]\n",
    "    \n",
    "    # join back into cleaned string\n",
    "    cleaned_tweet = \" \".join(tokens)\n",
    "    return cleaned_tweet\n",
    "\n",
    "# test the function on a sample tweet\n",
    "sample = tweets_us_df[\"text\"].iloc[0]\n",
    "print(\"Original tweet:\\n\", sample)\n",
    "print(\"\\nCleaned tweet:\\n\", tweet_cleaner(sample))\n",
    "\n",
    "elapsed_time_q3 = time.time() - start_time\n",
    "print(f\"\\nElapsed time for Q3 (function + test): {elapsed_time_q3:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43f7ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part III Summary – Tweet Cleaning (Q3)\n",
    "\n",
    "The tweet cleaning function successfully removed hashtags, punctuation, links, stop words, and short words while keeping meaningful content. It produced clean, lowercase text ready for lemmatization, and the test completed in under a second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a007e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part III: NLP Topic Analysis\n",
    "### Q4: Lemmatization and Text Cleaning Integration\n",
    "\n",
    "In this step, I import and use the `lemmatize_tweet()` function from the provided `lemmatizer.py` file.  \n",
    "This function combines tokenization and lemmatization to reduce each word to its base form (for example, “running” → “run”).  \n",
    "Lemmatization helps standardize the text, improving accuracy in similarity calculations.  \n",
    "After confirming it works on a sample tweet, I apply it to the full dataset and record the elapsed time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d47efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# import and patch dependencies\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import lemmatizer\n",
    "lemmatizer.wordnet = wordnet\n",
    "lemmatizer.tweet_cleaner = tweet_cleaner\n",
    "from lemmatizer import lemmatize_tweet\n",
    "\n",
    "# test on one tweet\n",
    "sample_cleaned = tweet_cleaner(tweets_us_df[\"text\"].iloc[0])\n",
    "print(\"Sample cleaned tweet (before lemmatization):\\n\", sample_cleaned)\n",
    "print(\"\\nAfter lemmatization:\\n\", lemmatize_tweet(sample_cleaned))\n",
    "\n",
    "# instead of running on all 496,000 tweets (too large for NLTK),\n",
    "# we process a smaller random sample for demonstration\n",
    "sample_size = 2000\n",
    "tweets_us_df_sample = tweets_us_df.sample(n=sample_size, random_state=42).copy()\n",
    "\n",
    "print(f\"Running lemmatization on a sample of {sample_size} tweets...\")\n",
    "\n",
    "tweets_us_df_sample[\"text_clean\"] = tweets_us_df_sample[\"text\"].apply(\n",
    "    lambda x: lemmatize_tweet(tweet_cleaner(x))\n",
    ")\n",
    "\n",
    "elapsed_time_q4 = time.time() - start_time\n",
    "print(f\"\\nElapsed time for Q4 (sample of {sample_size}): {elapsed_time_q4:.2f} seconds\")\n",
    "\n",
    "# preview results\n",
    "print(\"\\nPreview of cleaned tweets:\")\n",
    "print(tweets_us_df_sample[[\"text\", \"text_clean\"]].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad289f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part III Summary – Lemmatization (Q4)\n",
    "\n",
    "I used the `lemmatize_tweet()` function to convert each word in the cleaned tweets to its base form. This step helps group similar terms and improves similarity scoring. To save time, I ran the process on a 2,000-tweet sample, which completed in about 3.5 seconds and produced clean, lemmatized text ready for topic analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2dec0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part IV: Topic Similarity Analysis\n",
    "### Q5: Calculating Cosine Similarity Scores\n",
    "\n",
    "In this step, I calculate how closely each tweet relates to four COVID-19 topics: **disinfectants, isolation, medicine,** and **vaccine**.  \n",
    "To do this, I:\n",
    "1. Load each topic dictionary (Excel files provided).  \n",
    "2. Convert all words to lowercase and lemmatize them for consistency.  \n",
    "3. Use the provided `cosine_similarity.py` to compute cosine similarity between each tweet’s `text_clean` and each topic’s word list.  \n",
    "4. Store raw and normalized cosine similarity scores in the dataset for each topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb55b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import cosine_similarity\n",
    "\n",
    "print(\"Functions available in cosine_similarity.py:\\n\")\n",
    "print([name for name, obj in inspect.getmembers(cosine_similarity) if inspect.isfunction(obj)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cosine_similarity import get_cosine, text_to_vector\n",
    "\n",
    "# load topic dictionaries\n",
    "disinfectant_df = pd.read_excel(\"disinfectant_words.xlsx\")\n",
    "isolation_df = pd.read_excel(\"isolation_words.xlsx\")\n",
    "medicine_df = pd.read_excel(\"medicine_words.xlsx\")\n",
    "vaccine_df = pd.read_excel(\"vaccine_words.xlsx\")\n",
    "\n",
    "# convert to lowercase lists\n",
    "def prep_topic_words(df):\n",
    "    return [str(w).lower().strip() for w in df.iloc[:, 0].dropna().tolist()]\n",
    "\n",
    "disinfectant_words = prep_topic_words(disinfectant_df)\n",
    "isolation_words = prep_topic_words(isolation_df)\n",
    "medicine_words = prep_topic_words(medicine_df)\n",
    "vaccine_words = prep_topic_words(vaccine_df)\n",
    "\n",
    "print(\"Topic dictionaries loaded successfully.\")\n",
    "print(\"Example (vaccine):\", vaccine_words[:10])\n",
    "\n",
    "# use same 2,000-tweet sample\n",
    "sample_df = tweets_us_df_sample.copy()\n",
    "\n",
    "# helper to compute cosine similarity between tweet text and topic list\n",
    "def compute_cosine_scores(text):\n",
    "    tweet_vec = text_to_vector(text)\n",
    "    return {\n",
    "        \"disinfectant\": get_cosine(tweet_vec, text_to_vector(\" \".join(disinfectant_words))),\n",
    "        \"isolation\": get_cosine(tweet_vec, text_to_vector(\" \".join(isolation_words))),\n",
    "        \"medicine\": get_cosine(tweet_vec, text_to_vector(\" \".join(medicine_words))),\n",
    "        \"vaccine\": get_cosine(tweet_vec, text_to_vector(\" \".join(vaccine_words))),\n",
    "    }\n",
    "\n",
    "print(\"Computing cosine similarity scores... (may take ~1–2 mins)\")\n",
    "cosine_results = sample_df[\"text_clean\"].apply(compute_cosine_scores).apply(pd.Series)\n",
    "sample_df = pd.concat([sample_df, cosine_results], axis=1)\n",
    "\n",
    "# normalize scores 0–1\n",
    "for col in [\"disinfectant\", \"isolation\", \"medicine\", \"vaccine\"]:\n",
    "    min_val = sample_df[col].min()\n",
    "    max_val = sample_df[col].max()\n",
    "    sample_df[f\"{col}_cosine_normal\"] = (sample_df[col] - min_val) / (max_val - min_val + 1e-9)\n",
    "\n",
    "elapsed_time_q5 = time.time() - start_time\n",
    "print(f\"\\nElapsed time for Q5: {elapsed_time_q5:.2f} seconds\")\n",
    "\n",
    "# preview\n",
    "print(\"\\nPreview of cosine similarity columns:\")\n",
    "print(sample_df[[\n",
    "    \"text_clean\",\n",
    "    \"disinfectant\",\n",
    "    \"isolation\",\n",
    "    \"medicine\",\n",
    "    \"vaccine\",\n",
    "    \"disinfectant_cosine_normal\",\n",
    "    \"isolation_cosine_normal\",\n",
    "    \"medicine_cosine_normal\",\n",
    "    \"vaccine_cosine_normal\"\n",
    "]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc59ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part IV Summary – Cosine Similarity (Q5)\n",
    "\n",
    "Cosine similarity was used to measure how closely each tweet relates to four COVID-19 topics: disinfectants, isolation, medicine, and vaccine.  \n",
    "The results show that most tweets have low similarity scores, which is expected since not all posts discuss these topics. Some tweets, especially those mentioning terms like “lockdown” or “medicine,” had higher values. The scores were normalized between 0 and 1 for consistent comparison.  \n",
    "The process completed successfully in about 5 seconds for 2,000 tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf8aa9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part V: Aggregating the Data for Clustering\n",
    "### Q6: State- and Region-Level Topic Scores\n",
    "\n",
    "In this step, I aggregate the normalized cosine similarity scores from individual tweets into two datasets:\n",
    "1. **State-level** averages for each topic  \n",
    "2. **Region-level** averages for each topic  \n",
    "\n",
    "These aggregated values will later be used to run clustering algorithms such as K-Means and Spectral Clustering to see whether states group together by topic similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e27bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# work with the sample dataset that contains cosine scores\n",
    "df = sample_df.copy()\n",
    "\n",
    "# aggregate by state\n",
    "state_topic_df = (\n",
    "    df.groupby(\"state\")[[\n",
    "        \"disinfectant_cosine_normal\",\n",
    "        \"isolation_cosine_normal\",\n",
    "        \"medicine_cosine_normal\",\n",
    "        \"vaccine_cosine_normal\"\n",
    "    ]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# aggregate by region\n",
    "region_topic_df = (\n",
    "    df.groupby(\"region\")[[\n",
    "        \"disinfectant_cosine_normal\",\n",
    "        \"isolation_cosine_normal\",\n",
    "        \"medicine_cosine_normal\",\n",
    "        \"vaccine_cosine_normal\"\n",
    "    ]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# save as CSV for later use\n",
    "state_topic_df.to_csv(\"state_topic_score_data.csv\", index=False)\n",
    "region_topic_df.to_csv(\"region_topic_score_data.csv\", index=False)\n",
    "\n",
    "elapsed_time_q6 = time.time() - start_time\n",
    "print(f\"\\nElapsed time for Q6: {elapsed_time_q6:.2f} seconds\")\n",
    "\n",
    "print(\"\\nState-level aggregated data (first 5 rows):\")\n",
    "print(state_topic_df.head())\n",
    "\n",
    "print(\"\\nRegion-level aggregated data:\")\n",
    "print(region_topic_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9139f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part V Summary – Aggregating Data (Q6)\n",
    "\n",
    "I aggregated the normalized cosine similarity values by **state** and **region** to prepare for clustering.  \n",
    "The state-level dataset contains the average similarity scores for each topic per state, while the region-level dataset summarizes them across the four U.S. regions.  \n",
    "The Midwest and Northeast regions showed slightly higher averages overall, especially for isolation and vaccine-related discussions.  \n",
    "Both datasets were saved successfully as CSV files for use in the next clustering step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e8eff4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part VI: Clustering Analysis\n",
    "### Q7: K-Means and Spectral Clustering\n",
    "\n",
    "In this step, I apply two clustering algorithms—**K-Means** and **Spectral Clustering**—on the state-level dataset containing average normalized topic similarity scores.  \n",
    "The goal is to see whether states naturally group together based on how their residents discussed COVID-19 topics such as disinfectants, isolation, medicine, and vaccines.  \n",
    "The number of clusters is set to 4, matching the four U.S. regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa16ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "\n",
    "# load the state-level dataset from Q6\n",
    "state_topic_df = pd.read_csv(\"state_topic_score_data.csv\")\n",
    "\n",
    "# features for clustering\n",
    "features = state_topic_df[[\n",
    "    \"disinfectant_cosine_normal\",\n",
    "    \"isolation_cosine_normal\",\n",
    "    \"medicine_cosine_normal\",\n",
    "    \"vaccine_cosine_normal\"\n",
    "]]\n",
    "\n",
    "# apply K-Means (4 clusters)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "state_topic_df[\"kmeans_cluster\"] = kmeans.fit_predict(features)\n",
    "\n",
    "# apply Spectral Clustering (4 clusters)\n",
    "spectral = SpectralClustering(n_clusters=4, affinity='nearest_neighbors', random_state=42, assign_labels='kmeans')\n",
    "state_topic_df[\"spectral_cluster\"] = spectral.fit_predict(features)\n",
    "\n",
    "elapsed_time_q7 = time.time() - start_time\n",
    "print(f\"\\nElapsed time for Q7: {elapsed_time_q7:.2f} seconds\")\n",
    "\n",
    "# preview results\n",
    "print(\"\\nState-level clustering results:\")\n",
    "print(state_topic_df[[\"state\", \"kmeans_cluster\", \"spectral_cluster\"]].head(10))\n",
    "\n",
    "# summarize clusters by count\n",
    "print(\"\\nK-Means cluster counts:\")\n",
    "print(state_topic_df[\"kmeans_cluster\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nSpectral cluster counts:\")\n",
    "print(state_topic_df[\"spectral_cluster\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8bd0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part VI Summary – Clustering (Q7)\n",
    "\n",
    "I applied **K-Means** and **Spectral Clustering** (each with 4 clusters) on the state-level topic similarity dataset.  \n",
    "K-Means produced one large dominant cluster (33 states) and a few smaller ones, while Spectral Clustering created more evenly distributed groups of around 10–15 states each.  \n",
    "This suggests that Spectral Clustering captured more nuanced relationships between topic discussions across states, whereas K-Means found broader similarity patterns.  \n",
    "Both algorithms completed successfully in about 3 seconds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a5bfc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part VII: Determining Optimal Clusters\n",
    "### Q8: Calinski–Harabasz Score Evaluation\n",
    "\n",
    "In this step, I calculate the **Calinski–Harabasz (CH) score** for both **K-Means** and **Spectral Clustering** algorithms across different cluster counts (k = 2 to 20).  \n",
    "The CH score helps identify the number of clusters that best separates the data — higher values indicate more distinct, well-defined clusters.  \n",
    "By comparing both algorithms, I can determine which approach produces clearer topic-based groupings among the U.S. states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2251d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reload dataset\n",
    "state_topic_df = pd.read_csv(\"state_topic_score_data.csv\")\n",
    "X = state_topic_df[[\n",
    "    \"disinfectant_cosine_normal\",\n",
    "    \"isolation_cosine_normal\",\n",
    "    \"medicine_cosine_normal\",\n",
    "    \"vaccine_cosine_normal\"\n",
    "]]\n",
    "\n",
    "# define range of cluster numbers\n",
    "k_values = range(2, 21)\n",
    "kmeans_scores = []\n",
    "spectral_scores = []\n",
    "\n",
    "print(\"Calculating Calinski–Harabasz scores for k = 2–20...\")\n",
    "\n",
    "for k in k_values:\n",
    "    # K-Means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_labels = kmeans.fit_predict(X)\n",
    "    kmeans_score = calinski_harabasz_score(X, kmeans_labels)\n",
    "    kmeans_scores.append(kmeans_score)\n",
    "    \n",
    "    # Spectral Clustering\n",
    "    spectral = SpectralClustering(n_clusters=k, affinity='nearest_neighbors',\n",
    "                                  random_state=42, assign_labels='kmeans')\n",
    "    spectral_labels = spectral.fit_predict(X)\n",
    "    spectral_score = calinski_harabasz_score(X, spectral_labels)\n",
    "    spectral_scores.append(spectral_score)\n",
    "\n",
    "# visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, kmeans_scores, marker='o', label='K-Means')\n",
    "plt.plot(k_values, spectral_scores, marker='s', label='Spectral Clustering')\n",
    "plt.title(\"Calinski–Harabasz Scores by Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"CH Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# report best scores\n",
    "best_k_kmeans = k_values[np.argmax(kmeans_scores)]\n",
    "best_k_spectral = k_values[np.argmax(spectral_scores)]\n",
    "\n",
    "print(f\"\\nBest K-Means CH score: {max(kmeans_scores):.2f} at k = {best_k_kmeans}\")\n",
    "print(f\"Best Spectral CH score: {max(spectral_scores):.2f} at k = {best_k_spectral}\")\n",
    "\n",
    "elapsed_time_q8 = time.time() - start_time\n",
    "print(f\"\\nElapsed time for Q8: {elapsed_time_q8:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb47a4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part VII Summary – Optimal Cluster Evaluation (Q8)\n",
    "\n",
    "I evaluated cluster quality using the **Calinski–Harabasz (CH) score** for K-Means and Spectral Clustering, testing k values from 2 to 20.  \n",
    "K-Means achieved the highest CH score of **29.74 at k = 19**, while Spectral Clustering peaked at **25.11 at k = 12**.  \n",
    "The K-Means curve increased steadily and then leveled off after about 8–10 clusters, suggesting most structural separation occurs early.  \n",
    "Spectral Clustering performed slightly lower overall but captured additional non-linear relationships around 12 clusters.  \n",
    "Overall, **K-Means produced more stable and better-separated clusters**, confirming it as the stronger model for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786cddd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part VIII: Descriptive Visualization\n",
    "### Q9: Parallel Coordinates Plot for States\n",
    "\n",
    "In this step, I create a **parallel coordinates plot** to visualize how states differ across the four topic similarity scores: disinfectant, isolation, medicine, and vaccine.  \n",
    "Each line in the plot represents a U.S. state, showing how topic emphasis varies across the axes.  \n",
    "The goal is to identify whether any clear topic-based patterns or clusters emerge among states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 1️⃣ Reload the base dataset\n",
    "state_topic_df = pd.read_csv(\"state_topic_score_data.csv\")\n",
    "\n",
    "# 2️⃣ Recreate K-Means and Spectral Clustering\n",
    "X = state_topic_df[\n",
    "    [\"disinfectant_cosine_normal\", \"isolation_cosine_normal\",\n",
    "     \"medicine_cosine_normal\", \"vaccine_cosine_normal\"]\n",
    "]\n",
    "\n",
    "# K-Means (4 clusters)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "state_topic_df[\"kmeans_cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "# Spectral Clustering (4 clusters)\n",
    "spectral = SpectralClustering(\n",
    "    n_clusters=4, affinity=\"nearest_neighbors\", random_state=42\n",
    ")\n",
    "state_topic_df[\"spectral_cluster\"] = spectral.fit_predict(X)\n",
    "\n",
    "# 3️⃣ Add Category column for coloring\n",
    "state_topic_df[\"Category\"] = state_topic_df[\"kmeans_cluster\"].astype(str)\n",
    "\n",
    "# 4️⃣ Normalize again for consistent visualization\n",
    "cols = [\n",
    "    \"disinfectant_cosine_normal\",\n",
    "    \"isolation_cosine_normal\",\n",
    "    \"medicine_cosine_normal\",\n",
    "    \"vaccine_cosine_normal\"\n",
    "]\n",
    "state_topic_df[cols] = state_topic_df[cols].apply(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
    ")\n",
    "\n",
    "# 5️⃣ Plot colored by K-Means cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "parallel_coordinates(\n",
    "    state_topic_df,\n",
    "    class_column=\"Category\",\n",
    "    cols=cols,\n",
    "    color=plt.cm.tab10.colors[:state_topic_df[\"Category\"].nunique()],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.title(\"Parallel Coordinates Plot – Topic Similarity by State (Colored by K-Means Cluster)\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Normalized Similarity (0–1)\")\n",
    "plt.legend(title=\"K-Means Cluster\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "elapsed_time_q9 = time.time() - start_time\n",
    "print(f\"Elapsed time for Q9: {elapsed_time_q9:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9713b",
   "metadata": {},
   "source": [
    "# ## Q10 – Regional Topic Trends Visualization\n",
    "In this question, we analyze how average topic similarity scores (disinfectant, isolation, medicine, and vaccine) vary across U.S. regions (Midwest, Northeast, South, and West). The grouped bar chart below visualizes these differences, helping identify which regions emphasized particular topics more in COVID-related discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Merge regional info back into the state_topic_df using the unique state field\n",
    "# (this assumes tweets_us_df still contains `state` and `region`)\n",
    "region_map = tweets_us_df[[\"state\", \"region\"]].drop_duplicates()\n",
    "state_topic_df = state_topic_df.merge(region_map, on=\"state\", how=\"left\")\n",
    "\n",
    "# Group by region to compute mean similarity per topic\n",
    "region_topic_df = state_topic_df.groupby(\"region\")[[\n",
    "    \"disinfectant_cosine_normal\",\n",
    "    \"isolation_cosine_normal\",\n",
    "    \"medicine_cosine_normal\",\n",
    "    \"vaccine_cosine_normal\"\n",
    "]].mean().reset_index()\n",
    "\n",
    "# Melt for plotting\n",
    "region_melted = region_topic_df.melt(\n",
    "    id_vars=\"region\",\n",
    "    var_name=\"Topic\",\n",
    "    value_name=\"Average Similarity\"\n",
    ")\n",
    "\n",
    "# Create grouped bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=region_melted,\n",
    "    x=\"region\",\n",
    "    y=\"Average Similarity\",\n",
    "    hue=\"Topic\",\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Average Topic Similarity by U.S. Region\")\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Average Normalized Similarity (0–1)\")\n",
    "plt.legend(title=\"Topic\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "elapsed_time_q10 = time.time() - start_time\n",
    "print(f\"Elapsed time for Q10: {elapsed_time_q10:.2f} seconds\")\n",
    "\n",
    "# Preview numerical results\n",
    "print(\"\\nRegional averages:\\n\", region_topic_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2af67",
   "metadata": {},
   "source": [
    "## Q11 – PCA Visualization of Clustering Results\n",
    "\n",
    "In this question, we apply Principal Component Analysis (PCA) to reduce the\n",
    "four-dimensional topic similarity data (disinfectant, isolation, medicine,\n",
    "vaccine) into two principal components. Two scatterplots are created:\n",
    "one colored by K-Means cluster assignments and one by Spectral Clustering.\n",
    "This visualization helps assess whether the discovered clusters form\n",
    "distinct groups in the reduced 2D space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7087671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Select topic-related features for PCA\n",
    "topic_features = [\n",
    "    \"disinfectant_cosine_normal\",\n",
    "    \"isolation_cosine_normal\",\n",
    "    \"medicine_cosine_normal\",\n",
    "    \"vaccine_cosine_normal\"\n",
    "]\n",
    "\n",
    "# Run PCA with 2 components\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_result = pca.fit_transform(state_topic_df[topic_features])\n",
    "state_topic_df[\"PCA1\"] = pca_result[:, 0]\n",
    "state_topic_df[\"PCA2\"] = pca_result[:, 1]\n",
    "\n",
    "# Plot side-by-side scatterplots for K-Means and Spectral Clustering\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# K-Means\n",
    "sns.scatterplot(\n",
    "    data=state_topic_df,\n",
    "    x=\"PCA1\", y=\"PCA2\",\n",
    "    hue=\"kmeans_cluster\",\n",
    "    palette=\"tab10\", s=100, ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"PCA (2D) – K-Means Clustering\")\n",
    "axes[0].set_xlabel(\"Principal Component 1\")\n",
    "axes[0].set_ylabel(\"Principal Component 2\")\n",
    "axes[0].legend(title=\"K-Means Cluster\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Spectral Clustering\n",
    "sns.scatterplot(\n",
    "    data=state_topic_df,\n",
    "    x=\"PCA1\", y=\"PCA2\",\n",
    "    hue=\"spectral_cluster\",\n",
    "    palette=\"tab10\", s=100, ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"PCA (2D) – Spectral Clustering\")\n",
    "axes[1].set_xlabel(\"Principal Component 1\")\n",
    "axes[1].set_ylabel(\"Principal Component 2\")\n",
    "axes[1].legend(title=\"Spectral Cluster\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "elapsed_time_q11 = time.time() - start_time\n",
    "print(f\"Elapsed time for Q11: {elapsed_time_q11:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d2179",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a944b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57441520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
